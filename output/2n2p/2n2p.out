here
here
Time taken to read file: 0.57s

Start of epoch 0
At step 10, Update gradient for each batch taken: 0.17s
At step 20, Update gradient for each batch taken: 0.17s
At step 30, Update gradient for each batch taken: 0.17s
At step 40, Update gradient for each batch taken: 0.17s
At step 50, Update gradient for each batch taken: 0.17s
At step 60, Update gradient for each batch taken: 0.19s
At step 70, Update gradient for each batch taken: 0.18s
At step 80, Update gradient for each batch taken: 0.18s
At step 90, Update gradient for each batch taken: 0.18s
At step 100, Update gradient for each batch taken: 0.18s
At step 110, Update gradient for each batch taken: 0.16s
At step 120, Update gradient for each batch taken: 0.16s
At step 130, Update gradient for each batch taken: 0.16s
At step 140, Update gradient for each batch taken: 0.16s
At step 150, Update gradient for each batch taken: 0.16s
At step 160, Update gradient for each batch taken: 0.16s
At step 170, Update gradient for each batch taken: 0.16s
At step 180, Update gradient for each batch taken: 0.16s
At step 190, Update gradient for each batch taken: 0.16s
At step 200, Update gradient for each batch taken: 0.17s
At step 210, Update gradient for each batch taken: 0.17s
At step 220, Update gradient for each batch taken: 0.16s
At step 230, Update gradient for each batch taken: 0.16s
Time taken: 82.07s
Training acc over epoch: 0.8484, Training loss (for one epoch) at step: 0.4697

Start of epoch 1
At step 10, Update gradient for each batch taken: 0.19s
At step 20, Update gradient for each batch taken: 0.20s
At step 30, Update gradient for each batch taken: 0.19s
At step 40, Update gradient for each batch taken: 0.19s
At step 50, Update gradient for each batch taken: 0.18s
At step 60, Update gradient for each batch taken: 0.19s
At step 70, Update gradient for each batch taken: 0.16s
At step 80, Update gradient for each batch taken: 0.17s
At step 90, Update gradient for each batch taken: 0.17s
At step 100, Update gradient for each batch taken: 0.16s
At step 110, Update gradient for each batch taken: 0.16s
At step 120, Update gradient for each batch taken: 0.15s
At step 130, Update gradient for each batch taken: 0.15s
At step 140, Update gradient for each batch taken: 0.15s
At step 150, Update gradient for each batch taken: 0.15s
At step 160, Update gradient for each batch taken: 0.13s
At step 170, Update gradient for each batch taken: 0.13s
At step 180, Update gradient for each batch taken: 0.15s
At step 190, Update gradient for each batch taken: 0.15s
At step 200, Update gradient for each batch taken: 0.15s
At step 210, Update gradient for each batch taken: 0.15s
At step 220, Update gradient for each batch taken: 0.15s
At step 230, Update gradient for each batch taken: 0.15s
Time taken: 82.00s
Training acc over epoch: 0.9534, Training loss (for one epoch) at step: 0.1533

Start of epoch 2
At step 10, Update gradient for each batch taken: 0.14s
At step 20, Update gradient for each batch taken: 0.13s
At step 30, Update gradient for each batch taken: 0.14s
At step 40, Update gradient for each batch taken: 0.14s
At step 50, Update gradient for each batch taken: 0.14s
At step 60, Update gradient for each batch taken: 0.15s
At step 70, Update gradient for each batch taken: 0.14s
At step 80, Update gradient for each batch taken: 0.13s
At step 90, Update gradient for each batch taken: 0.13s
At step 100, Update gradient for each batch taken: 0.13s
At step 110, Update gradient for each batch taken: 0.13s
At step 120, Update gradient for each batch taken: 0.13s
At step 130, Update gradient for each batch taken: 0.14s
At step 140, Update gradient for each batch taken: 0.13s
At step 150, Update gradient for each batch taken: 0.13s
At step 160, Update gradient for each batch taken: 0.13s
At step 170, Update gradient for each batch taken: 0.13s
At step 180, Update gradient for each batch taken: 0.13s
At step 190, Update gradient for each batch taken: 0.13s
At step 200, Update gradient for each batch taken: 0.13s
At step 210, Update gradient for each batch taken: 0.13s
At step 220, Update gradient for each batch taken: 0.13s
At step 230, Update gradient for each batch taken: 0.13s
Time taken: 81.99s
Training acc over epoch: 0.9661, Training loss (for one epoch) at step: 0.1117

Start of epoch 3
At step 10, Update gradient for each batch taken: 0.13s
At step 20, Update gradient for each batch taken: 0.13s
At step 30, Update gradient for each batch taken: 0.13s
At step 40, Update gradient for each batch taken: 0.13s
At step 50, Update gradient for each batch taken: 0.13s
At step 60, Update gradient for each batch taken: 0.13s
At step 70, Update gradient for each batch taken: 0.13s
At step 80, Update gradient for each batch taken: 0.13s
At step 90, Update gradient for each batch taken: 0.13s
At step 100, Update gradient for each batch taken: 0.13s
At step 110, Update gradient for each batch taken: 0.13s
At step 120, Update gradient for each batch taken: 0.13s
At step 130, Update gradient for each batch taken: 0.13s
At step 140, Update gradient for each batch taken: 0.13s
At step 150, Update gradient for each batch taken: 0.13s
At step 160, Update gradient for each batch taken: 0.13s
At step 170, Update gradient for each batch taken: 0.13s
At step 180, Update gradient for each batch taken: 0.13s
At step 190, Update gradient for each batch taken: 0.13s
At step 200, Update gradient for each batch taken: 0.13s
At step 210, Update gradient for each batch taken: 0.13s
At step 220, Update gradient for each batch taken: 0.13s
At step 230, Update gradient for each batch taken: 0.13s
Time taken: 81.99s
Training acc over epoch: 0.9743, Training loss (for one epoch) at step: 0.0827

Start of epoch 4
At step 10, Update gradient for each batch taken: 0.13s
At step 20, Update gradient for each batch taken: 0.14s
At step 30, Update gradient for each batch taken: 0.13s
At step 40, Update gradient for each batch taken: 0.13s
At step 50, Update gradient for each batch taken: 0.13s
At step 60, Update gradient for each batch taken: 0.13s
At step 70, Update gradient for each batch taken: 0.14s
At step 80, Update gradient for each batch taken: 0.13s
At step 90, Update gradient for each batch taken: 0.13s
At step 100, Update gradient for each batch taken: 0.13s
At step 110, Update gradient for each batch taken: 0.13s
At step 120, Update gradient for each batch taken: 0.13s
At step 130, Update gradient for each batch taken: 0.13s
At step 140, Update gradient for each batch taken: 0.12s
At step 150, Update gradient for each batch taken: 0.12s
At step 160, Update gradient for each batch taken: 0.12s
At step 170, Update gradient for each batch taken: 0.12s
At step 180, Update gradient for each batch taken: 0.12s
At step 190, Update gradient for each batch taken: 0.12s
At step 200, Update gradient for each batch taken: 0.12s
At step 210, Update gradient for each batch taken: 0.12s
At step 220, Update gradient for each batch taken: 0.12s
At step 230, Update gradient for each batch taken: 0.12s
Time taken: 81.98s
Training acc over epoch: 0.9786, Training loss (for one epoch) at step: 0.0684
Test loss: 0.05207967013120651
Test accuracy: 0.9847999811172485
Total run time: 415.21s